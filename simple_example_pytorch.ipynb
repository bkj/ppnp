{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from ppnp.pytorch import PPNP\n",
    "from ppnp.pytorch.training import train_model\n",
    "from ppnp.pytorch.earlystopping import stopping_args\n",
    "from ppnp.pytorch.propagation import PPRExact, PPRPowerIteration\n",
    "from ppnp.data.io import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "        format='%(asctime)s: %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "\n",
    "First we need to load the dataset we want to train on. The datasets used are in the `SparseGraph` format. This is just a class providing the adjacency, attribute and label matrices in a dense (`np.ndarray`) or sparse (`scipy.sparse.csr_matrix`) matrix format and some (in principle unnecessary) convenience functions. If you want to use external datasets, you can e.g. use the `networkx_to_sparsegraph` method in `ppnp.data.io` for converting NetworkX graphs to our SparseGraph format.\n",
    "\n",
    "The four datasets from the paper (Cora-ML, Citeseer, PubMed and MS Academic) can be found in the directory `data`.\n",
    "\n",
    "For this example we choose the Cora-ML graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Undirected, unweighted and connected SparseGraph with 15962 edges (no self-loops). Data: adj_matrix (2810x2810), attr_matrix (2810x2879), labels (2810), node_names (2810), attr_names (2879), class_names (7)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_name = 'cora_ml'\n",
    "graph = load_dataset(graph_name)\n",
    "graph.standardize(select_lcc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up propagation\n",
    "\n",
    "Next we need to set up the proper propagation scheme. In the paper we've introduced the exact PPR propagation used in PPNP and the PPR power iteration propagation used in APPNP.\n",
    "\n",
    "Here we use the hyperparameters from the paper. Note that we should use a different `alpha = 0.2` for MS Academic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_ppnp = PPRExact(graph.adj_matrix, alpha=0.1)\n",
    "prop_appnp = PPRPowerIteration(graph.adj_matrix, alpha=0.1, niter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose model hyperparameters\n",
    "\n",
    "Now we choose the hyperparameters. These are the ones used in the paper for all datasets.\n",
    "\n",
    "Note that we choose the propagation for APPNP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    'hiddenunits': [64],\n",
    "    'drop_prob': 0.5,\n",
    "    'propagation': prop_appnp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "\n",
    "Now we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_split_args = {'ntrain_per_class': 20, 'nstopping': 500, 'nknown': 1500, 'seed': 2413340114}\n",
    "reg_lambda = 5e-3\n",
    "learning_rate = 0.01\n",
    "\n",
    "test = False\n",
    "device = 'cuda'\n",
    "print_interval = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-10 18:30:13: PPNP: {'hiddenunits': [64], 'drop_prob': 0.5, 'propagation': PPRPowerIteration()}\n",
      "2019-11-10 18:30:13: PyTorch seed: 1419657858\n",
      "2019-11-10 18:30:16: Epoch 0: Train loss = 2.00, train acc = 15.7, early stopping loss = 1.96, early stopping acc = 33.6 (0.587 sec)\n",
      "2019-11-10 18:30:17: Epoch 20: Train loss = 1.94, train acc = 62.1, early stopping loss = 1.95, early stopping acc = 46.8 (0.583 sec)\n",
      "2019-11-10 18:30:17: Epoch 40: Train loss = 1.90, train acc = 70.0, early stopping loss = 1.95, early stopping acc = 52.4 (0.554 sec)\n",
      "2019-11-10 18:30:18: Epoch 60: Train loss = 1.83, train acc = 86.4, early stopping loss = 1.93, early stopping acc = 65.4 (0.559 sec)\n",
      "2019-11-10 18:30:18: Epoch 80: Train loss = 1.76, train acc = 87.1, early stopping loss = 1.89, early stopping acc = 70.4 (0.559 sec)\n",
      "2019-11-10 18:30:19: Epoch 100: Train loss = 1.68, train acc = 89.3, early stopping loss = 1.83, early stopping acc = 74.2 (0.558 sec)\n",
      "2019-11-10 18:30:19: Epoch 120: Train loss = 1.57, train acc = 95.7, early stopping loss = 1.77, early stopping acc = 75.8 (0.562 sec)\n",
      "2019-11-10 18:30:20: Epoch 140: Train loss = 1.49, train acc = 92.1, early stopping loss = 1.70, early stopping acc = 78.6 (0.559 sec)\n",
      "2019-11-10 18:30:21: Epoch 160: Train loss = 1.42, train acc = 95.0, early stopping loss = 1.65, early stopping acc = 78.4 (0.555 sec)\n",
      "2019-11-10 18:30:21: Epoch 180: Train loss = 1.35, train acc = 95.7, early stopping loss = 1.60, early stopping acc = 77.8 (0.578 sec)\n",
      "2019-11-10 18:30:22: Epoch 200: Train loss = 1.27, train acc = 94.3, early stopping loss = 1.55, early stopping acc = 80.2 (0.555 sec)\n",
      "2019-11-10 18:30:22: Epoch 220: Train loss = 1.25, train acc = 96.4, early stopping loss = 1.50, early stopping acc = 79.8 (0.561 sec)\n",
      "2019-11-10 18:30:23: Epoch 240: Train loss = 1.17, train acc = 96.4, early stopping loss = 1.47, early stopping acc = 79.8 (0.553 sec)\n",
      "2019-11-10 18:30:23: Epoch 260: Train loss = 1.16, train acc = 95.0, early stopping loss = 1.43, early stopping acc = 80.6 (0.555 sec)\n",
      "2019-11-10 18:30:24: Epoch 280: Train loss = 1.11, train acc = 96.4, early stopping loss = 1.39, early stopping acc = 80.6 (0.554 sec)\n",
      "2019-11-10 18:30:24: Epoch 300: Train loss = 1.05, train acc = 97.9, early stopping loss = 1.37, early stopping acc = 80.6 (0.553 sec)\n",
      "2019-11-10 18:30:25: Epoch 320: Train loss = 1.01, train acc = 97.9, early stopping loss = 1.34, early stopping acc = 81.0 (0.553 sec)\n",
      "2019-11-10 18:30:26: Epoch 340: Train loss = 1.00, train acc = 98.6, early stopping loss = 1.31, early stopping acc = 81.2 (0.572 sec)\n",
      "2019-11-10 18:30:26: Epoch 360: Train loss = 0.94, train acc = 98.6, early stopping loss = 1.29, early stopping acc = 80.8 (0.552 sec)\n",
      "2019-11-10 18:30:27: Epoch 380: Train loss = 0.94, train acc = 100.0, early stopping loss = 1.27, early stopping acc = 81.4 (0.552 sec)\n",
      "2019-11-10 18:30:27: Epoch 400: Train loss = 0.92, train acc = 99.3, early stopping loss = 1.25, early stopping acc = 81.6 (0.552 sec)\n",
      "2019-11-10 18:30:28: Epoch 420: Train loss = 0.90, train acc = 99.3, early stopping loss = 1.24, early stopping acc = 81.2 (0.552 sec)\n",
      "2019-11-10 18:30:28: Epoch 440: Train loss = 0.91, train acc = 98.6, early stopping loss = 1.23, early stopping acc = 80.4 (0.555 sec)\n",
      "2019-11-10 18:30:29: Epoch 460: Train loss = 0.83, train acc = 97.9, early stopping loss = 1.21, early stopping acc = 81.0 (0.553 sec)\n",
      "2019-11-10 18:30:29: Epoch 480: Train loss = 0.84, train acc = 99.3, early stopping loss = 1.20, early stopping acc = 82.4 (0.551 sec)\n",
      "2019-11-10 18:30:30: Epoch 500: Train loss = 0.80, train acc = 99.3, early stopping loss = 1.17, early stopping acc = 80.8 (0.553 sec)\n",
      "2019-11-10 18:30:31: Epoch 520: Train loss = 0.79, train acc = 98.6, early stopping loss = 1.17, early stopping acc = 80.4 (0.568 sec)\n",
      "2019-11-10 18:30:31: Epoch 540: Train loss = 0.78, train acc = 99.3, early stopping loss = 1.15, early stopping acc = 80.8 (0.552 sec)\n",
      "2019-11-10 18:30:32: Epoch 560: Train loss = 0.77, train acc = 99.3, early stopping loss = 1.13, early stopping acc = 81.6 (0.551 sec)\n",
      "2019-11-10 18:30:32: Epoch 580: Train loss = 0.75, train acc = 100.0, early stopping loss = 1.12, early stopping acc = 82.2 (0.552 sec)\n",
      "2019-11-10 18:30:33: Epoch 600: Train loss = 0.72, train acc = 99.3, early stopping loss = 1.11, early stopping acc = 81.8 (0.555 sec)\n",
      "2019-11-10 18:30:33: Epoch 620: Train loss = 0.75, train acc = 98.6, early stopping loss = 1.11, early stopping acc = 80.6 (0.552 sec)\n",
      "2019-11-10 18:30:34: Epoch 640: Train loss = 0.74, train acc = 99.3, early stopping loss = 1.10, early stopping acc = 81.0 (0.551 sec)\n",
      "2019-11-10 18:30:34: Epoch 660: Train loss = 0.68, train acc = 99.3, early stopping loss = 1.10, early stopping acc = 81.4 (0.553 sec)\n",
      "2019-11-10 18:30:35: Epoch 680: Train loss = 0.68, train acc = 98.6, early stopping loss = 1.07, early stopping acc = 81.8 (0.569 sec)\n",
      "2019-11-10 18:30:36: Epoch 700: Train loss = 0.67, train acc = 99.3, early stopping loss = 1.05, early stopping acc = 82.0 (0.551 sec)\n",
      "2019-11-10 18:30:36: Epoch 720: Train loss = 0.66, train acc = 99.3, early stopping loss = 1.05, early stopping acc = 82.8 (0.552 sec)\n",
      "2019-11-10 18:30:37: Epoch 740: Train loss = 0.67, train acc = 99.3, early stopping loss = 1.04, early stopping acc = 82.8 (0.551 sec)\n",
      "2019-11-10 18:30:37: Epoch 760: Train loss = 0.64, train acc = 97.9, early stopping loss = 1.04, early stopping acc = 81.4 (0.551 sec)\n",
      "2019-11-10 18:30:38: Epoch 780: Train loss = 0.65, train acc = 100.0, early stopping loss = 1.03, early stopping acc = 82.6 (0.552 sec)\n",
      "2019-11-10 18:30:38: Epoch 800: Train loss = 0.63, train acc = 99.3, early stopping loss = 1.01, early stopping acc = 83.0 (0.552 sec)\n",
      "2019-11-10 18:30:39: Epoch 820: Train loss = 0.64, train acc = 99.3, early stopping loss = 1.01, early stopping acc = 82.0 (0.551 sec)\n",
      "2019-11-10 18:30:39: Epoch 840: Train loss = 0.61, train acc = 100.0, early stopping loss = 1.00, early stopping acc = 83.4 (0.551 sec)\n",
      "2019-11-10 18:30:40: Epoch 860: Train loss = 0.61, train acc = 99.3, early stopping loss = 1.00, early stopping acc = 81.8 (0.563 sec)\n",
      "2019-11-10 18:30:41: Epoch 880: Train loss = 0.59, train acc = 98.6, early stopping loss = 1.00, early stopping acc = 81.4 (0.552 sec)\n",
      "2019-11-10 18:30:41: Epoch 900: Train loss = 0.60, train acc = 99.3, early stopping loss = 1.01, early stopping acc = 82.4 (0.551 sec)\n",
      "2019-11-10 18:30:42: Epoch 920: Train loss = 0.59, train acc = 100.0, early stopping loss = 1.00, early stopping acc = 82.0 (0.552 sec)\n",
      "2019-11-10 18:30:42: Epoch 940: Train loss = 0.59, train acc = 100.0, early stopping loss = 0.99, early stopping acc = 81.6 (0.552 sec)\n",
      "2019-11-10 18:30:43: Epoch 960: Train loss = 0.59, train acc = 98.6, early stopping loss = 0.98, early stopping acc = 81.4 (0.552 sec)\n",
      "2019-11-10 18:30:43: Epoch 980: Train loss = 0.58, train acc = 100.0, early stopping loss = 0.98, early stopping acc = 82.0 (0.552 sec)\n",
      "2019-11-10 18:30:44: Epoch 1000: Train loss = 0.57, train acc = 98.6, early stopping loss = 0.99, early stopping acc = 80.8 (0.553 sec)\n",
      "2019-11-10 18:30:44: Epoch 1020: Train loss = 0.54, train acc = 100.0, early stopping loss = 0.97, early stopping acc = 82.0 (0.567 sec)\n",
      "2019-11-10 18:30:45: Epoch 1040: Train loss = 0.55, train acc = 100.0, early stopping loss = 0.95, early stopping acc = 82.4 (0.551 sec)\n",
      "2019-11-10 18:30:46: Epoch 1060: Train loss = 0.53, train acc = 99.3, early stopping loss = 0.96, early stopping acc = 81.4 (0.551 sec)\n",
      "2019-11-10 18:30:46: Epoch 1080: Train loss = 0.56, train acc = 100.0, early stopping loss = 0.94, early stopping acc = 81.4 (0.551 sec)\n",
      "2019-11-10 18:30:47: Epoch 1100: Train loss = 0.54, train acc = 100.0, early stopping loss = 0.96, early stopping acc = 81.0 (0.553 sec)\n",
      "2019-11-10 18:30:47: Epoch 1120: Train loss = 0.53, train acc = 100.0, early stopping loss = 0.95, early stopping acc = 83.0 (0.551 sec)\n",
      "2019-11-10 18:30:48: Epoch 1140: Train loss = 0.52, train acc = 99.3, early stopping loss = 0.96, early stopping acc = 80.8 (0.551 sec)\n",
      "2019-11-10 18:30:48: Epoch 1160: Train loss = 0.51, train acc = 99.3, early stopping loss = 0.93, early stopping acc = 82.4 (0.551 sec)\n",
      "2019-11-10 18:30:49: Epoch 1180: Train loss = 0.49, train acc = 100.0, early stopping loss = 0.92, early stopping acc = 81.6 (0.551 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-10 18:30:49: Epoch 1200: Train loss = 0.51, train acc = 100.0, early stopping loss = 0.92, early stopping acc = 81.8 (0.563 sec)\n",
      "2019-11-10 18:30:50: Epoch 1220: Train loss = 0.50, train acc = 100.0, early stopping loss = 0.92, early stopping acc = 81.0 (0.553 sec)\n",
      "2019-11-10 18:30:50: Epoch 1240: Train loss = 0.51, train acc = 100.0, early stopping loss = 0.92, early stopping acc = 81.0 (0.551 sec)\n",
      "2019-11-10 18:30:51: Epoch 1260: Train loss = 0.49, train acc = 100.0, early stopping loss = 0.90, early stopping acc = 82.6 (0.551 sec)\n",
      "2019-11-10 18:30:52: Epoch 1280: Train loss = 0.48, train acc = 99.3, early stopping loss = 0.91, early stopping acc = 81.4 (0.551 sec)\n",
      "2019-11-10 18:30:52: Epoch 1300: Train loss = 0.51, train acc = 100.0, early stopping loss = 0.90, early stopping acc = 81.2 (0.551 sec)\n",
      "2019-11-10 18:30:53: Epoch 1320: Train loss = 0.50, train acc = 98.6, early stopping loss = 0.93, early stopping acc = 81.0 (0.552 sec)\n",
      "2019-11-10 18:30:53: Epoch 1340: Train loss = 0.48, train acc = 100.0, early stopping loss = 0.92, early stopping acc = 82.6 (0.551 sec)\n",
      "2019-11-10 18:30:54: Epoch 1360: Train loss = 0.46, train acc = 99.3, early stopping loss = 0.87, early stopping acc = 83.6 (0.551 sec)\n",
      "2019-11-10 18:30:54: Epoch 1380: Train loss = 0.47, train acc = 100.0, early stopping loss = 0.88, early stopping acc = 82.0 (0.562 sec)\n",
      "2019-11-10 18:30:55: Epoch 1400: Train loss = 0.47, train acc = 100.0, early stopping loss = 0.91, early stopping acc = 81.2 (0.551 sec)\n",
      "2019-11-10 18:30:55: Epoch 1420: Train loss = 0.46, train acc = 100.0, early stopping loss = 0.91, early stopping acc = 80.2 (0.551 sec)\n",
      "2019-11-10 18:30:56: Epoch 1440: Train loss = 0.47, train acc = 99.3, early stopping loss = 0.86, early stopping acc = 83.6 (0.551 sec)\n",
      "2019-11-10 18:30:57: Epoch 1460: Train loss = 0.45, train acc = 100.0, early stopping loss = 0.88, early stopping acc = 81.2 (0.552 sec)\n",
      "2019-11-10 18:30:57: Epoch 1480: Train loss = 0.43, train acc = 100.0, early stopping loss = 0.92, early stopping acc = 79.8 (0.552 sec)\n",
      "2019-11-10 18:30:58: Epoch 1500: Train loss = 0.48, train acc = 98.6, early stopping loss = 0.87, early stopping acc = 82.4 (0.551 sec)\n",
      "2019-11-10 18:30:58: Epoch 1520: Train loss = 0.44, train acc = 100.0, early stopping loss = 0.85, early stopping acc = 83.6 (0.551 sec)\n",
      "2019-11-10 18:30:59: Epoch 1540: Train loss = 0.43, train acc = 100.0, early stopping loss = 0.88, early stopping acc = 82.0 (0.563 sec)\n",
      "2019-11-10 18:30:59: Epoch 1560: Train loss = 0.45, train acc = 99.3, early stopping loss = 0.89, early stopping acc = 81.6 (0.552 sec)\n",
      "2019-11-10 18:31:00: Epoch 1580: Train loss = 0.42, train acc = 98.6, early stopping loss = 0.86, early stopping acc = 82.2 (0.551 sec)\n",
      "2019-11-10 18:31:00: Epoch 1600: Train loss = 0.43, train acc = 100.0, early stopping loss = 0.85, early stopping acc = 82.4 (0.551 sec)\n",
      "2019-11-10 18:31:01: Epoch 1620: Train loss = 0.44, train acc = 99.3, early stopping loss = 0.84, early stopping acc = 83.0 (0.551 sec)\n",
      "2019-11-10 18:31:02: Epoch 1640: Train loss = 0.43, train acc = 99.3, early stopping loss = 0.86, early stopping acc = 83.6 (0.552 sec)\n",
      "2019-11-10 18:31:02: Epoch 1660: Train loss = 0.43, train acc = 99.3, early stopping loss = 0.88, early stopping acc = 82.6 (0.552 sec)\n",
      "2019-11-10 18:31:03: Epoch 1680: Train loss = 0.41, train acc = 100.0, early stopping loss = 0.85, early stopping acc = 83.0 (0.546 sec)\n",
      "2019-11-10 18:31:03: Epoch 1700: Train loss = 0.41, train acc = 99.3, early stopping loss = 0.86, early stopping acc = 82.6 (0.546 sec)\n",
      "2019-11-10 18:31:04: Epoch 1720: Train loss = 0.43, train acc = 98.6, early stopping loss = 0.82, early stopping acc = 82.8 (0.557 sec)\n",
      "2019-11-10 18:31:04: Epoch 1740: Train loss = 0.41, train acc = 100.0, early stopping loss = 0.83, early stopping acc = 82.2 (0.546 sec)\n",
      "2019-11-10 18:31:05: Epoch 1760: Train loss = 0.40, train acc = 100.0, early stopping loss = 0.84, early stopping acc = 82.2 (0.546 sec)\n",
      "2019-11-10 18:31:05: Epoch 1780: Train loss = 0.38, train acc = 100.0, early stopping loss = 0.84, early stopping acc = 83.0 (0.546 sec)\n",
      "2019-11-10 18:31:06: Epoch 1800: Train loss = 0.39, train acc = 100.0, early stopping loss = 0.82, early stopping acc = 83.0 (0.546 sec)\n",
      "2019-11-10 18:31:06: Epoch 1820: Train loss = 0.40, train acc = 99.3, early stopping loss = 0.83, early stopping acc = 83.0 (0.546 sec)\n",
      "2019-11-10 18:31:07: Epoch 1840: Train loss = 0.40, train acc = 100.0, early stopping loss = 0.86, early stopping acc = 81.6 (0.546 sec)\n",
      "2019-11-10 18:31:08: Epoch 1860: Train loss = 0.41, train acc = 99.3, early stopping loss = 0.82, early stopping acc = 83.2 (0.547 sec)\n",
      "2019-11-10 18:31:08: Epoch 1880: Train loss = 0.37, train acc = 99.3, early stopping loss = 0.80, early stopping acc = 83.0 (0.556 sec)\n",
      "2019-11-10 18:31:09: Epoch 1900: Train loss = 0.39, train acc = 100.0, early stopping loss = 0.83, early stopping acc = 81.6 (0.546 sec)\n",
      "2019-11-10 18:31:09: Epoch 1920: Train loss = 0.39, train acc = 100.0, early stopping loss = 0.82, early stopping acc = 83.0 (0.546 sec)\n",
      "2019-11-10 18:31:10: Epoch 1940: Train loss = 0.39, train acc = 100.0, early stopping loss = 0.82, early stopping acc = 83.0 (0.545 sec)\n",
      "2019-11-10 18:31:10: Epoch 1960: Train loss = 0.39, train acc = 99.3, early stopping loss = 0.83, early stopping acc = 82.2 (0.546 sec)\n",
      "2019-11-10 18:31:11: Last epoch: 1972, best epoch: 1853 (55.159 sec)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PPNP:\n\tWhile copying the parameter named \"propagation.A_hat\", whose dimensions in the model are torch.Size([2810, 2810]) and whose dimensions in the checkpoint are torch.Size([2810, 2810]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-91e15796942c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m result = train_model(\n\u001b[1;32m      2\u001b[0m         \u001b[0mgraph_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPPNP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_lambda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         idx_split_args, stopping_args, test, device, None, print_interval)\n\u001b[0m",
      "\u001b[0;32m~/software/ppnp/ppnp/pytorch/training.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(name, model_class, graph, model_args, learning_rate, reg_lambda, idx_split_args, stopping_args, test, device, torch_seed, print_interval)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;31m# Load best model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mstopping_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_mat_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stopping'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 845\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PPNP:\n\tWhile copying the parameter named \"propagation.A_hat\", whose dimensions in the model are torch.Size([2810, 2810]) and whose dimensions in the checkpoint are torch.Size([2810, 2810])."
     ]
    }
   ],
   "source": [
    "result = train_model(\n",
    "        graph_name, PPNP, graph, model_args, learning_rate, reg_lambda,\n",
    "        idx_split_args, stopping_args, test, device, None, print_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
