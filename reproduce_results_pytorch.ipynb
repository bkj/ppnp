{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from ppnp.pytorch import PPNP\n",
    "from ppnp.pytorch.training import train_model\n",
    "from ppnp.pytorch.earlystopping import stopping_args\n",
    "from ppnp.pytorch.propagation import PPRExact, PPRPowerIteration\n",
    "from ppnp.data.io import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "        format='%(asctime)s: %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        level=logging.INFO + 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Undirected, unweighted and connected SparseGraph with 15962 edges (no self-loops). Data: adj_matrix (2810x2810), attr_matrix (2810x2879), labels (2810), node_names (2810), attr_names (2879), class_names (7)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_name = 'cora_ml'\n",
    "graph = load_dataset(graph_name)\n",
    "graph.standardize(select_lcc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up data splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need to decide whether to use the test or validation set. Be mindful that we can only look at the test set exactly _once_ and then can't change any hyperparameters oder model details, no matter what. Everything else would cause overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the seeds for the dataset splits used in the paper for test/validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seeds = [\n",
    "        2144199730,  794209841, 2985733717, 2282690970, 1901557222,\n",
    "        2009332812, 2266730407,  635625077, 3538425002,  960893189,\n",
    "        497096336, 3940842554, 3594628340,  948012117, 3305901371,\n",
    "        3644534211, 2297033685, 4092258879, 2590091101, 1694925034]\n",
    "val_seeds = [\n",
    "        2413340114, 3258769933, 1789234713, 2222151463, 2813247115,\n",
    "        1920426428, 4272044734, 2092442742, 841404887, 2188879532,\n",
    "        646784207, 1633698412, 2256863076,  374355442,  289680769,\n",
    "        4281139389, 4263036964,  900418539,  119332950, 1628837138]\n",
    "\n",
    "if test:\n",
    "    seeds = test_seeds\n",
    "else:\n",
    "    seeds = val_seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can choose the remaining settings for the training/early stopping/validation(test) split. These are the ones chosen in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if graph_name == 'microsoft_academic':\n",
    "    nknown = 5000\n",
    "else:\n",
    "    nknown = 1500\n",
    "    \n",
    "idx_split_args = {'ntrain_per_class': 20, 'nstopping': 500, 'nknown': nknown}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up propagation\n",
    "\n",
    "Next we need to set up the proper pmropagation scheme. In the paper we've introduced the exact PPR propagation used in PPNP and the PPR power iteration propagation used in APPNP.\n",
    "\n",
    "We use the hyperparameters from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if graph_name == 'microsoft_academic':\n",
    "    alpha = 0.2\n",
    "else:\n",
    "    alpha = 0.1\n",
    "\n",
    "prop_ppnp = PPRExact(graph.adj_matrix, alpha=alpha)\n",
    "prop_appnp = PPRPowerIteration(graph.adj_matrix, alpha=alpha, niter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose model hyperparameters\n",
    "\n",
    "Now we choose the hyperparameters. These are the ones used in the paper for all datasets.\n",
    "\n",
    "Note that we choose the propagation for APPNP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_lambda    = 5e-3\n",
    "learning_rate = 0.01\n",
    "\n",
    "tf_model_args = {\n",
    "    'hiddenunits': [64],\n",
    "    'reg_lambda': reg_lambda,\n",
    "    'learning_rate': learning_rate,\n",
    "    'keep_prob': 0.5,\n",
    "    'propagation': prop_appnp}\n",
    "\n",
    "pytorch_model_args = {\n",
    "    'hiddenunits': [64],\n",
    "    'drop_prob': 0.5,\n",
    "    'propagation': prop_appnp\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "\n",
    "First we set the remaining settings for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter_per_seed = 5\n",
    "save_result = False\n",
    "print_interval = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 20 different seeds for splitting and 5 iterations (different random initializations) per split, so we train 100 times altogether. This will take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 15:10:49: Iteration 1 of 100\n",
      "                     ------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'train_loss': 1.9992010593414307, 'train_acc': 0.14285714285714285, 'stop_loss': 1.958577036857605, 'stop_acc': 0.462}\n",
      "{'epoch': 100, 'train_loss': 1.6320810317993164, 'train_acc': 0.7285714285714285, 'stop_loss': 1.8021395206451416, 'stop_acc': 0.634}\n",
      "{'epoch': 200, 'train_loss': 1.2943041324615479, 'train_acc': 0.9214285714285714, 'stop_loss': 1.5530493259429932, 'stop_acc': 0.772}\n",
      "{'epoch': 300, 'train_loss': 1.079572319984436, 'train_acc': 0.9714285714285714, 'stop_loss': 1.3974164724349976, 'stop_acc': 0.79}\n",
      "{'epoch': 400, 'train_loss': 0.9509291648864746, 'train_acc': 0.9642857142857143, 'stop_loss': 1.2893569469451904, 'stop_acc': 0.794}\n",
      "{'epoch': 500, 'train_loss': 0.8628926277160645, 'train_acc': 0.9857142857142858, 'stop_loss': 1.229329228401184, 'stop_acc': 0.794}\n",
      "{'epoch': 600, 'train_loss': 0.7725332975387573, 'train_acc': 0.9785714285714285, 'stop_loss': 1.1681612730026245, 'stop_acc': 0.798}\n",
      "{'epoch': 700, 'train_loss': 0.6923733353614807, 'train_acc': 0.9928571428571429, 'stop_loss': 1.1274793148040771, 'stop_acc': 0.802}\n",
      "{'epoch': 800, 'train_loss': 0.6780211925506592, 'train_acc': 0.9928571428571429, 'stop_loss': 1.0789363384246826, 'stop_acc': 0.802}\n",
      "{'epoch': 900, 'train_loss': 0.6295711994171143, 'train_acc': 0.9857142857142858, 'stop_loss': 1.046868920326233, 'stop_acc': 0.804}\n",
      "{'epoch': 1000, 'train_loss': 0.5908129215240479, 'train_acc': 0.9857142857142858, 'stop_loss': 1.0257714986801147, 'stop_acc': 0.792}\n",
      "{'epoch': 1100, 'train_loss': 0.5699880719184875, 'train_acc': 0.9857142857142858, 'stop_loss': 1.0680017471313477, 'stop_acc': 0.776}\n",
      "{'epoch': 1200, 'train_loss': 0.5389014482498169, 'train_acc': 1.0, 'stop_loss': 0.9851654171943665, 'stop_acc': 0.806}\n",
      "{'epoch': 1300, 'train_loss': 0.5186856389045715, 'train_acc': 0.9857142857142858, 'stop_loss': 0.9556052684783936, 'stop_acc': 0.806}\n",
      "{'epoch': 1400, 'train_loss': 0.4823923110961914, 'train_acc': 1.0, 'stop_loss': 0.958306074142456, 'stop_acc': 0.798}\n",
      "{'epoch': 1500, 'train_loss': 0.4773728847503662, 'train_acc': 1.0, 'stop_loss': 0.9840577840805054, 'stop_acc': 0.786}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 15:11:21: Iteration 2 of 100\n",
      "                     ------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1600, 'train_loss': 0.4465727210044861, 'train_acc': 0.9928571428571429, 'stop_loss': 0.9352177381515503, 'stop_acc': 0.8}\n",
      "{'epoch': 0, 'train_loss': 1.9993115663528442, 'train_acc': 0.14285714285714285, 'stop_loss': 1.9587469100952148, 'stop_acc': 0.424}\n",
      "{'epoch': 100, 'train_loss': 1.6304686069488525, 'train_acc': 0.8142857142857143, 'stop_loss': 1.8586913347244263, 'stop_acc': 0.584}\n"
     ]
    }
   ],
   "source": [
    "# results = []\n",
    "used_seeds = []\n",
    "niter_tot = niter_per_seed * len(seeds)\n",
    "i_tot = 0\n",
    "for seed in seeds[-40:]:\n",
    "    idx_split_args['seed'] = seed\n",
    "    for _ in range(niter_per_seed):\n",
    "        i_tot += 1\n",
    "        logging_string = f\"Iteration {i_tot} of {niter_tot}\"\n",
    "        logging.log(22, logging_string + \"\\n                     \"+ '-' * len(logging_string))\n",
    "    \n",
    "#         # tensorflow\n",
    "#         result = train_model(\n",
    "#                 name=graph_name,\n",
    "#                 model_class=PPNP,\n",
    "#                 graph=graph, \n",
    "#                 build_args=tf_model_args, \n",
    "#                 idx_split_args=idx_split_args,\n",
    "#                 stopping_args=stopping_args, \n",
    "#                 test=test, \n",
    "#                 save_result=save_result, \n",
    "#                 tf_seed=None, \n",
    "#                 print_interval=print_interval\n",
    "#         )\n",
    "\n",
    "        # pytorch\n",
    "        model, result = train_model(\n",
    "            name=graph_name,\n",
    "            model_class=PPNP,\n",
    "            graph=graph,\n",
    "            model_args=pytorch_model_args,\n",
    "            reg_lambda=reg_lambda,\n",
    "            learning_rate=learning_rate,\n",
    "            idx_split_args=idx_split_args,\n",
    "            stopping_args=stopping_args,\n",
    "            test=test,\n",
    "            torch_seed=None,\n",
    "            print_interval=print_interval\n",
    "        )\n",
    "\n",
    "        results.append({})\n",
    "        results[-1]['stopping_accuracy'] = result['stopping_acc']\n",
    "#         results[-1]['stopping_f1_score'] = result['stopping']['f1_score']\n",
    "        results[-1]['valtest_accuracy']  = result['valtest_acc']\n",
    "#         results[-1]['valtest_f1_score']  = result['valtest']['f1_score']\n",
    "#         results[-1]['runtime']           = result['runtime']\n",
    "#         results[-1]['runtime_perepoch']  = result['runtime_perepoch']\n",
    "        results[-1]['split_seed']        = seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "To evaluate the data we use Pandas and Seaborn (for bootstrapping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8482856773191874"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame(results)\n",
    "result_df.head()\n",
    "\n",
    "result_df.valtest_accuracy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard deviation doesn't really say much about the uncertainty of our results and the standard error of the mean (SEM) assumes a normal distribution. So the best way to get a valid estimate for our results' uncertainty is via bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_uncertainty(values: np.ndarray, n_boot: int = 1000, ci: int = 95) -> dict:\n",
    "    stats = {}\n",
    "    stats['mean'] = values.mean()\n",
    "    boots_series = sns.algorithms.bootstrap(values, func=np.mean, n_boot=n_boot)\n",
    "    stats['CI'] = sns.utils.ci(boots_series, ci)\n",
    "    stats['uncertainty'] = np.max(np.abs(stats['CI'] - stats['mean']))\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/homedirs/klicpera/anaconda3/envs/graph/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    }
   ],
   "source": [
    "stopping_acc = calc_uncertainty(result_df['stopping_accuracy'])\n",
    "stopping_f1 = calc_uncertainty(result_df['stopping_f1_score'])\n",
    "valtest_acc = calc_uncertainty(result_df['valtest_accuracy'])\n",
    "valtest_f1 = calc_uncertainty(result_df['valtest_f1_score'])\n",
    "runtime = calc_uncertainty(result_df['runtime'])\n",
    "runtime_perepoch = calc_uncertainty(result_df['runtime_perepoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APPNP\n",
      "Early stopping: Accuracy: 84.17 ± 0.37%, F1 score: 0.8296 ± 0.0041\n",
      "Test: Accuracy: 85.05 ± 0.23%, F1 score: 0.8431 ± 0.0021\n",
      "Runtime: 45.661 ± 1.793 sec, per epoch: 32.72 ± 0.04ms\n"
     ]
    }
   ],
   "source": [
    "print(\"APPNP\\n\"\n",
    "      \"Early stopping: Accuracy: {:.2f} ± {:.2f}%, \"\n",
    "      \"F1 score: {:.4f} ± {:.4f}\\n\"\n",
    "      \"{}: Accuracy: {:.2f} ± {:.2f}%, \"\n",
    "      \"F1 score: {:.4f} ± {:.4f}\\n\"\n",
    "      \"Runtime: {:.3f} ± {:.3f} sec, per epoch: {:.2f} ± {:.2f}ms\"\n",
    "      .format(\n",
    "          stopping_acc['mean'] * 100,\n",
    "          stopping_acc['uncertainty'] * 100,\n",
    "          stopping_f1['mean'],\n",
    "          stopping_f1['uncertainty'],\n",
    "          'Test' if test else 'Validation',\n",
    "          valtest_acc['mean'] * 100,\n",
    "          valtest_acc['uncertainty'] * 100,\n",
    "          valtest_f1['mean'],\n",
    "          valtest_f1['uncertainty'],\n",
    "          runtime['mean'],\n",
    "          runtime['uncertainty'],\n",
    "          runtime_perepoch['mean'] * 1e3,\n",
    "          runtime_perepoch['uncertainty'] * 1e3,\n",
    "      ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
